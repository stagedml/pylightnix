REPL Demo
=========

[Complete source of the demo](./REPL.py)

This demonstration re-uses the setting of [MNIST demo](./MNIST.md). Here we show
how to use the collection of `repl_` functions to simplify debug sessions.

Consider the following chain of stages `Stage1 -> Stage2 -> Stage3`. Pylightnix
effectively caches the results of Stages 1 and 2 so if Stage 3 require debugging
(fix-and-rerun loop), it will not trigger their re-calculation.

But if Stage 3 is a long running job, and the problem appears at it's final
stage, we may want some way to pause it, do some manual checks then resume the
computation. Inserting calls to python debuggers like `ipdb` is one possible way
to pause the computation, but they typically offer only a limited shell and may
not provide full access to Python environment.

In Pylightnix we offer a more generic solution which 'drops' user into normal
IPython shell by storing the whole state of Pylightnix computation into a single
Puthon object.

Pylightnix uses Generators for internal event processing. It's generic functions
like `realize` hide this details, but the functions of the [Repl
module](./../../src/pylightnix/repl.py) provide API for non-trivial actions. It
allows us to pause the computation at the specific stage, do manual tweaks
either in IPython or in other Python shells, and finally continue the process.

During the pause, Pylightnix state is stored in `ReplHelper` object, either
global or specifically chosen by the user.


The setting
-----------

We repeat the MNIST definitions explained in the MNIST demo. First, the
necessary imports and initializations.

```python
import tensorflow as tf
assert tf.version.VERSION.startswith('2.1')

from os.path import join
from numpy import load as np_load
from tensorflow.keras.models import ( Sequential )
from tensorflow.keras.layers import ( Conv2D, MaxPool2D, Dropout, Flatten, Dense )
from tensorflow.keras.utils import ( to_categorical )
from tensorflow.keras.backend import image_data_format
from tensorflow.keras.callbacks import ModelCheckpoint

from pylightnix import ( Matcher, Build, Path, RefPath, Config, Manager, RRef,
    DRef, Context, build_path, build_outpath, build_cattrs, mkdrv, rref2path,
    mkconfig, mkbuild, match_best, build_wrapper_, tryread, fetchurl,
    store_initialize, realize, instantiate, mapbuild )

from typing import Any

store_initialize()
```

Next, we define two stages required to train the MNIST classifier. Note, how do
we split the realization into two subroutines: `mnist_train` and `mnist_eval`.
We will use it to track the problems.

```python

def fetchmnist(m:Manager)->DRef:
  return \
    fetchurl(m, name='mnist',
                mode='as-is',
                url='https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz',
                sha256='731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1')

class Model(Build):
  model:Sequential
  x_train:Any
  y_train:Any
  x_test:Any
  y_test:Any

def mnist_config(mnist:DRef)->Config:
  name = 'convnn_mnist'
  dataset:RefPath = [mnist, 'mnist.npz']
  learning_rate = 1e-3
  num_epoches = 1
  version = 7
  return mkconfig(locals())

def mnist_train(b:Model)->None:
  o = build_outpath(b)
  c = build_cattrs(b)

  with np_load(build_path(b, c.dataset), allow_pickle=True) as f:
    b.x_train, b.y_train = f['x_train'], f['y_train']
    b.x_test, b.y_test = f['x_test'], f['y_test']

  b.x_train = b.x_train.reshape(b.x_train.shape[0], 28, 28, 1).astype('float32') / 255
  b.y_train = to_categorical(b.y_train, 10)

  b.x_test = b.x_test.reshape(b.x_test.shape[0], 28, 28, 1).astype('float32') / 255
  b.y_test = to_categorical(b.y_test, 10)


  print('x_train shape:', b.x_train.shape)
  print(b.x_train.shape[0], 'train samples')
  print(b.x_test.shape[0], 'test samples')

  model = Sequential()
  b.model = model
  model.add(Conv2D(32, kernel_size=(3, 3), activation = 'relu', input_shape = (28,28,1)))
  model.add(Conv2D(64, (3, 3), activation = 'relu'))
  model.add(MaxPool2D(pool_size = (2,2)))
  model.add(Dropout(0.25))
  model.add(Flatten())
  model.add(Dense(128, activation = 'relu'))
  model.add(Dropout(0.5))
  model.add(Dense(10, activation = 'softmax'))

  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

  callbacks = [
    ModelCheckpoint(
      monitor='val_accuracy',
      filepath=join(o, "checkpoint.ckpt"),
      save_weights_only=True,
      save_best_only=True,
      verbose=True)]
  model.fit(b.x_train, b.y_train,
      batch_size=32,
      epochs=c.num_epoches,
      verbose=0,
      callbacks=callbacks,
      validation_split=0.2)

def mnist_eval(b:Model):
  o = build_outpath(b)
  b.model.load(join(o, "checkpoint.ckpt"))
  accuracy = b.model.evaluate(b.x_test, b.y_test, verbose = 0)[-1]
  print(accuracy)
  with open(join(o,'accuracy.txt'),'w') as f:
    f.write(str(accuracy))

def mnist_realize(b:Model):
  mnist_train(b)
  mnist_eval(b)

def convnn_mnist(m:Manager)->DRef:
  mnist = fetchmnist(m)
  return mkdrv(m, mnist_config(mnist), match_best('accuracy.txt'),
    build_wrapper_(mnist_realize, mapbuild(Model)))
```


The last definition of `convnn_mnist` define our target stage of MNIST
classifier.  Let's try to obtain it's realization

```python
realize(instantiate(convnn_mnist), force_rebuild=True)   # Spoiler: will fail
```

Oh no!!! We see a backtrace saying that TensorFlow models don't have `load`
method. It's strange, since the `save` method does exist, but probably TF team
has some reason for this design. Looking through the documentation shows us that
the right method to call is `load_weights`.

We may make this thistrivial fix in-place but for this document I have to define
a new funtion, containing the right call.

```python
def mnist_eval_correct(b:Model):
  o = build_outpath(b)
  b.model.load_weights(join(o, "checkpoint.ckpt"))
  accuracy = b.model.evaluate(b.x_test, b.y_test, verbose = 0)[-1]
  print(accuracy)
  with open(join(o,'accuracy.txt'),'w') as f:
    f.write(str(accuracy))

```

Now, we need to re-realize the derivation, but we are not sure it is the last
problem. In order to save time and prevent more re-runs we may decide to go
through the realization process 'by hands'. In order to do so, we typically do
the following:

1. Run IPython and load our defininitions into it. (Python notebook would
   probably also work)>
2. Run manually `repl_realize` function as shown below.

In contrast to normal `realize`, `repl_realize` pauses before the last
realization. Other interrupts may be programmed by passing a list of derivations
to pause via it's `force_interrupt` argument.

```python, term=True,wrap=False
from pylightnix import repl_realize, repl_build, repl_continueBuild

repl_realize(instantiate(convnn_mnist))
```

We see that Pylightnix returned `ReplHelper` object. This object holds the
paused state of Pylightnix. In particular, it contains `Build` object,
accessible by calling `repl_build` function. Note, that repl functions normally
use global `ReplHelper` if called without arguments. Global ReplHelper is a link
to the last `ReplHelper` created.

Now we could call our `mnist_train` and `mnist_eval_correct` as many times as we
want.

```python, term=True,wrap=False
mnist_train(repl_build())

mnist_eval_correct(repl_build())
```

When we are done (it turns out that we don't have any errors), we could call
`repl_continue` or it's simpler equivalent `repl_continueBuild` to take the
Pylightnix state out of the ReplHelper and continue it's normal execution

```python, wrap=False
rref=repl_continueBuild()
assert rref is not None
print(rref)
```

Repl continue build returns `RRef` if the build process is complete
or None if it has to make a new interrrupt. Since we programmed only one
interrupt, we now should have our realization.






